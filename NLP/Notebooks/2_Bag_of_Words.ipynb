{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('ml': conda)"
  },
  "interpreter": {
   "hash": "774e39f5f21f7b60062f3abb90b8afe0139135a3792f728ee4bd9a31085203c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Bag of Words\n",
    "Is a classical text representation technique that has been used\n",
    "commonly in NLP, especially in text classification problems.\n",
    "\n",
    "Represent the text under consideration as a bag (collection) of words while ignoring the order and context.  \n",
    "The basic intuition behind it is that it assumes that the text belonging to a given class in the dataset is characterized by a unique set of words. If two text pieces have nearly the same words, then they belong to the same bag (class). Thus, by analyzing the words present in a piece of text, one can identify the class (bag) it belongs to.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"images/bagofwords.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\nOur vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\nBoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\nBoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\nBow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#look at the documents list\n",
    "print(\"Our corpus: \", processed_docs)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#BoW with binary vectors\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "count_vect.fit(processed_docs)\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "source": [
    "## Bag of words Application"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         Review_text  Review_class\n",
       "0  A very, very, very slow-moving, aimless movie ...             0\n",
       "1  Not sure who was more lost - the flat characte...             0\n",
       "2  Attempting artiness with black & white and cle...             0\n",
       "3       Very little music or anything to speak of.               0\n",
       "4  The best scene in the movie was when Gerardo i...             1\n",
       "5  The rest of the movie lacks art, charm, meanin...             0\n",
       "6                                Wasted two hours.               0\n",
       "7  Saw the movie today and thought it was a good ...             1\n",
       "8                               A bit predictable.               0\n",
       "9  Loved the casting of Jimmy Buffet as the scien...             1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Review_text</th>\n      <th>Review_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A very, very, very slow-moving, aimless movie ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Not sure who was more lost - the flat characte...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Attempting artiness with black &amp; white and cle...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Very little music or anything to speak of.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The best scene in the movie was when Gerardo i...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The rest of the movie lacks art, charm, meanin...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Wasted two hours.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Saw the movie today and thought it was a good ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>A bit predictable.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Loved the casting of Jimmy Buffet as the scien...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data_imdb = pd.read_csv(\"imdb_labelled.txt\", delimiter='\\t', header=None)\n",
    "data_imdb.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "data_amazon = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "data_amazon.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "data_yelp = pd.read_csv(\"yelp_labelled.txt\", delimiter='\\t', header=None)\n",
    "data_yelp.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "data = pd.concat([data_imdb, data_amazon, data_yelp])\n",
    "data.shape\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['slowmov aimless movi distress drift young man',\n",
       " 'not sure lost flat charact audienc nearli half walk',\n",
       " 'attempt arti black white clever camera angl movi disappoint becam even ridicul act poor plot line almost nonexist',\n",
       " 'littl music anyth speak',\n",
       " 'best scene movi gerardo tri find song keep run head',\n",
       " 'rest movi lack art charm mean empti work guess empti',\n",
       " 'wast two hour',\n",
       " 'saw movi today thought good effort good messag kid',\n",
       " 'bit predict',\n",
       " 'love cast jimmi buffet scienc teacher']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "def clean_text(df):\n",
    "    all_reviews = list()\n",
    "    lines = df[\"Review_text\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        PS = PorterStemmer()\n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "        words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        all_reviews.append(words)\n",
    "    return all_reviews\n",
    "\n",
    "all_reviews = clean_text(data)\n",
    "all_reviews[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "stopwords.words(\"english\")[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2748, 4026)\n(2748,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer()   \n",
    "X = CV.fit_transform(all_reviews).toarray()\n",
    "y = data[\"Review_class\"].to_numpy()\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6236363636363637\n0.6877828054298643\n0.5922077922077922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# model = DecisionTreeClassifier(criterion=\"entropy\", random_state=41)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = GaussianNB()\n",
    "#model  =  LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}