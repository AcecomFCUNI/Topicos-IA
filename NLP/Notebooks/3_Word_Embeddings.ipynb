{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('ml': conda)"
  },
  "interpreter": {
   "hash": "774e39f5f21f7b60062f3abb90b8afe0139135a3792f728ee4bd9a31085203c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Word Embeddings\n",
    "- In 2013, a seminal work by Mikolov showed that their neural network–based word representation model known as “Word2vec,” based on “distributional similarity,” can capture word analogy relationships such as: King – Man + Woman ≈ Queen\n",
    "\n",
    "- Low dimensional vectors (50 - 500), dimension of 300 is the most used, also called embeddings.\n",
    "\n",
    "- Research gropus develop the following word embeddings: \n",
    "\n",
    "* Google: Word2vec  \n",
    "* Facebook: fastText  \n",
    "* Stanford: GloVe\n",
    "\n",
    "- Word2vec can be trained with 2 methods: CBOW and Skip-gram model  \n",
    "- fastText is trained using n-grams  \n",
    "- Glove is trained using co--ocurrence matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### How do we find the vector that best represents the meaning of the word?\n",
    "- Consider a large corpus of text as input and “learns” to  represent\n",
    "the words in a common vector space based on the contexts in which they appear in\n",
    "the corpus.  \n",
    "- Given a word w and the words appearing in its context C.\n",
    "- For every word w in corpus, we start with a vector v w initialized with random values. The Word2vec model refines the values in v_w by predicting v_w , given the vectors for words in the context 'C'. It does this using a two-layer neural network.\n",
    "\n",
    "Example:\n",
    " - My dog likes to play in the garden.\n",
    " - My cat likes to play in the kitchen.\n",
    " - I like to eat pizza."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"images/worde_3p.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "There are 3 scenarios to use word embeddings:  \n",
    "1. Train the word embeddings and the model at the same time.  \n",
    "2. First train the word embeddings with large dataset, the use it to train the model.  \n",
    "3. Load a pretrained word embeddings an use to train the model.   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"images/word_embeddings.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Pre-trained word embeddings\n",
    "\n",
    "- Training your own word embeddings is a pretty expensive process (in terms of both\n",
    "time and computing). However Pre-trained word embeddings are trained on a large corpus, such as Wikipedia, news articles, or even the entire web.  \n",
    "- Such embeddings can be thought of as a large collection of key-value pairs, where keys are the words in the vocabulary and values are their corresponding word vectors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model at GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "gn_vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "if not os.path.exists(\"GoogleNews-vectors-negative300.bin\"):\n",
    "    if not os.path.exists(\"../Ch2/GoogleNews-vectors-negative300.bin\"):\n",
    "        #Downloading the reqired model\n",
    "        if not os.path.exists(\"../Ch2/GoogleNews-vectors-negative300.bin.gz\"):\n",
    "            if not os.path.exists(\"GoogleNews-vectors-negative300.bin.gz\"):\n",
    "                wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")\n",
    "            gn_vec_zip_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "        else:\n",
    "            gn_vec_zip_path = \"../Ch2/GoogleNews-vectors-negative300.bin.gz\"\n",
    "        #Extracting the required model\n",
    "        with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n",
    "            with open(gn_vec_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    else:\n",
    "        gn_vec_path = \"../Ch2/\" + gn_vec_path\n",
    "\n",
    "print(f\"Model at {gn_vec_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings #This module ignores the various types of warnings generated\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "process = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "mem = virtual_memory()\n",
    "\n",
    "import time #This module is used to calculate the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory used in GB before Loading the Model: 4.21\n",
      "----------\n",
      "52.81 seconds taken to load\n",
      "----------\n",
      "Finished loading Word2Vec\n",
      "----------\n",
      "Memory used in GB after Loading the Model: 6.73\n",
      "----------\n",
      "Percentage increase in memory usage: 160.05% \n",
      "----------\n",
      "Numver of words in vocablulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrainedpath = gn_vec_path\n",
    "\n",
    "#Load W2V model. This will take some time, but it is a one time effort! \n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('-'*10)\n",
    "\n",
    "print('Finished loading Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numver of words in vocablulary: \",len(w2v_model.index_to_key)) #Number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('gorgeous', 0.8353005051612854),\n",
       " ('lovely', 0.8106936812400818),\n",
       " ('stunningly_beautiful', 0.7329413294792175),\n",
       " ('breathtakingly_beautiful', 0.7231340408325195),\n",
       " ('wonderful', 0.6854086518287659),\n",
       " ('fabulous', 0.6700063943862915),\n",
       " ('loveliest', 0.6612576246261597),\n",
       " ('prettiest', 0.6595001816749573),\n",
       " ('beatiful', 0.6593326330184937),\n",
       " ('magnificent', 0.6591402888298035)]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#Let us examine the model by knowing what the most similar words are, for a given word!\n",
    "w2v_model.most_similar('beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('montreal', 0.6984112858772278),\n",
       " ('vancouver', 0.6587257385253906),\n",
       " ('nyc', 0.6248832941055298),\n",
       " ('alberta', 0.6179691553115845),\n",
       " ('boston', 0.611499547958374),\n",
       " ('calgary', 0.61032634973526),\n",
       " ('edmonton', 0.6100260615348816),\n",
       " ('canadian', 0.5944076776504517),\n",
       " ('chicago', 0.5911980271339417),\n",
       " ('springfield', 0.5888351798057556)]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#Let us try with another word! \n",
    "w2v_model.most_similar('toronto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#What is the vector representation for a word? \n",
    "w2v_model['computer'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "w2v_model['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-07-04 13:16:15.616178: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2021-07-04 13:16:15.616272: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2021-07-04 13:16:15.616286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Collecting en-core-web-md==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl (47.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 47.1 MB 138 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from en-core-web-md==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: jinja2 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10.3)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.2)\n",
      "Requirement already satisfied: setuptools in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (44.0.0.post20200102)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (20.3)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.46.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.18.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.22.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.6)\n",
      "Requirement already satisfied: six in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.13.0)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: more-itertools in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/jnavio/anaconda3/envs/ml/lib/python3.7/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/jnavio/anaconda3/envs/ml/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1.37 s, sys: 165 ms, total: 1.53 s\nWall time: 1.64 s\n[-1.12055197e-01  2.26087615e-01 -5.15111461e-02 -1.21812008e-01\n  4.13958639e-01 -8.56475979e-02 -2.84600933e-03 -2.26096585e-01\n  6.98113963e-02  2.27946019e+00 -4.49774921e-01 -6.39050007e-02\n -1.80326015e-01 -8.79765972e-02  9.93399299e-04 -1.57384202e-01\n -1.23817801e-01  1.54990411e+00  2.00794004e-02  1.38399601e-01\n -1.48897991e-01 -2.23025799e-01 -1.48171991e-01  4.68924567e-02\n -3.17026004e-02  1.19096041e-02 -6.10985979e-02  9.57068056e-02\n  9.37099904e-02  1.70955807e-01 -9.29740071e-03  7.88536817e-02\n  1.74508005e-01 -1.04450598e-01  1.04872189e-01 -1.16961405e-01\n  6.23028055e-02 -2.23016590e-01 -1.44107476e-01 -2.03423887e-01\n  2.61404991e-01  2.43404001e-01  1.51980996e-01 -1.12484001e-01\n  1.18055798e-01 -9.51323956e-02  8.66319984e-02 -2.54322797e-01\n  3.84932049e-02  1.18278004e-01 -3.21602583e-01  3.73764008e-01\n  1.13018408e-01 -8.05834010e-02  1.84921592e-01  9.38879885e-03\n  1.22166201e-01 -3.24288011e-02  1.01590000e-01 -1.56877995e-01\n -2.57006437e-02  1.63392588e-01  1.06118001e-01  2.25193188e-01\n  8.06204006e-02 -1.21081993e-01 -1.52107209e-01  8.25726017e-02\n -6.09899946e-02  1.44145802e-01  2.01554038e-02  2.54258011e-02\n  1.06071997e-02  6.37948066e-02  1.10551611e-01 -6.40176088e-02\n -6.36451989e-02 -9.99798030e-02 -7.01020136e-02  3.09334368e-01\n  5.68300001e-02  3.63879651e-03 -1.65255398e-01  2.98442870e-01\n  4.01660334e-03 -1.73631594e-01  5.15965708e-02  1.61811799e-01\n  2.20304996e-01 -8.29614028e-02 -2.64678001e-01  2.44114012e-01\n  3.48895532e-03 -1.57521993e-01  1.67974800e-01  1.05541132e-01\n -1.31224409e-01  7.17941970e-02  1.39708191e-01 -1.95359858e-03\n -8.55428055e-02  1.20119795e-01 -6.84404075e-02  5.14601183e-04\n -2.86250003e-02 -1.10662603e+00  2.02491835e-01 -1.50410801e-01\n  6.51507173e-03 -3.30360234e-03  1.21523812e-01 -1.61614027e-02\n -1.43233404e-01 -9.88139957e-02 -2.17486005e-02  1.81988999e-01\n  8.85506049e-02  2.72242010e-01 -7.73219988e-02  1.43622067e-02\n -1.57062009e-01  4.01146002e-02  3.90305184e-02 -1.42812401e-01\n -2.08329991e-01  9.64459926e-02  1.42821997e-01 -1.94155991e-01\n  5.37982993e-02 -1.00471973e-02  1.94714032e-02 -9.83514041e-02\n -4.17162031e-02  1.23069003e-01  1.68428212e-01 -1.17991492e-01\n -2.56704390e-01 -1.89464003e-01  9.22677964e-02 -1.72503412e-01\n -1.11929202e+00  6.42500073e-03  3.51435989e-01  8.19059983e-02\n  4.92408946e-02 -1.80243999e-01  1.82863399e-01  8.92240033e-02\n  2.47399211e-01  2.74492018e-02 -2.49322020e-02  2.35055804e-01\n  8.12319964e-02 -1.86482631e-02 -1.06439434e-01  5.28851971e-02\n -1.02569997e-01  1.35777995e-01 -2.32603997e-01  9.24602076e-02\n  1.92440599e-01  1.48551196e-01  5.57186007e-02  3.97088043e-02\n -6.74048066e-03  9.73687991e-02  2.62231939e-02 -8.26509967e-02\n  1.30085424e-01 -1.38572007e-01 -4.11808006e-02 -4.13070023e-02\n -3.41880023e-02  1.28202796e-01 -6.66912049e-02 -7.41944537e-02\n -5.87003939e-02  1.36300415e-01  1.67494014e-01  1.71119809e-01\n  1.18692197e-01  2.30142009e-02 -2.06086040e-02 -3.85930002e-01\n -1.17673976e-02 -7.34595209e-02 -3.43096368e-02 -7.80718103e-02\n -2.81003956e-02 -7.30765983e-02 -2.21649408e-01 -1.02057599e-01\n  5.11020012e-02 -9.07440037e-02 -4.69896048e-02 -2.10200553e-03\n  1.05816983e-01  4.79107983e-02  1.03080198e-01 -8.96641985e-02\n  8.85651931e-02 -9.09178331e-02 -5.16167991e-02  1.50742605e-01\n  3.07500213e-01 -4.05239780e-03  1.04269005e-01  3.55780013e-02\n  1.16165996e-01 -2.97939777e-03 -1.42966792e-01  5.00957891e-02\n -1.08308598e-01  1.68199837e-03  1.36314392e-01  1.48694202e-01\n -3.17817986e-01  1.21000603e-01 -1.59556001e-01  7.51644000e-02\n -1.03386201e-01  1.10754207e-01  8.20529982e-02 -6.02059904e-03\n  1.35578603e-01 -4.08943966e-02  6.05328009e-02  1.03734590e-01\n -6.22724071e-02  2.30276197e-01  1.30762011e-01  1.51950002e-01\n  7.40183964e-02 -1.84507206e-01 -1.33174613e-01 -1.49338007e-01\n  1.19309977e-01 -2.41554022e-01 -1.00904807e-01  1.54562384e-01\n -7.63845369e-02  1.66379198e-01  2.20374197e-01  1.58361979e-02\n  1.80677801e-01 -1.77342609e-01  2.22857997e-01 -2.99477577e-01\n -1.53620601e-01 -2.67919600e-01  1.56353399e-01 -1.74718007e-01\n  1.83644608e-01  1.28259212e-01 -6.30084053e-02  2.68236816e-01\n  2.10368007e-01 -4.73994762e-02 -1.09680817e-01  1.62620202e-01\n  8.96113962e-02  1.50361210e-01 -1.55037967e-02  1.50141995e-02\n  1.76618043e-02 -2.28057191e-01  7.85290003e-02 -4.59632799e-02\n  1.98103897e-02 -1.71379801e-02 -1.45676598e-01 -3.32076550e-02\n -2.09102005e-01 -2.48584002e-01 -8.51256028e-02  4.25900035e-02\n -1.33966401e-01  2.89979968e-02  2.10713193e-01 -1.86206046e-02\n  1.71603993e-01  2.21868396e-01 -2.10479975e-01  1.49794608e-01\n -1.10692397e-01 -4.47340589e-03  5.13652042e-02 -7.27116019e-02\n  6.07413948e-02 -8.13369974e-02 -1.94639400e-01 -5.06809242e-02\n  6.40980080e-02 -2.20814198e-01  2.96969917e-02  1.53438210e-01\n -2.18270030e-02 -1.93358198e-01 -2.26220220e-01  1.84093148e-01]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "%time nlp = spacy.load('en_core_web_md')\n",
    "# process a sentence using the model\n",
    "mydoc = nlp(\"Canada is a large country\")\n",
    "#Get a vector for individual words\n",
    "#print(doc[0].vector) #vector for 'Canada', the first word in the text \n",
    "print(mydoc.vector) #Averaged vector for the entire sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(300,)\nCanada\n(300,)\n"
     ]
    }
   ],
   "source": [
    "print(mydoc.vector.shape)\n",
    "print(mydoc[0])\n",
    "print(mydoc[0].vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "temp = nlp('practicalnlp is a newword')\n",
    "temp[0].vector"
   ]
  },
  {
   "source": [
    "### Training WordEmbeddings\n",
    "\n",
    "There are 2 variants:  \n",
    "• Continuous bag of words (CBOW)  \n",
    "• SkipGram"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "CBOW:  \n",
    "<img src=\"images/CBOW.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Example with k = 2  \n",
    "<img src=\"images/cbow_2.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "CBOW Model:\n",
    "\n",
    "<img src=\"images/cbow_model.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Distributed Representations Beyond Words and Characters (Doc2vec)\n",
    "Problem of word2vec is that learned representations for words, and we aggregated them\n",
    "to form text representations  but that they do not take the context of words\n",
    "into account.  \n",
    "Example: Apple has more similarity with Microsoft than orange.\n",
    "\n",
    "Doc2vec allows us to directly learn the representations for texts of arbitrary lengths (phrases, sentences, paragraphs, and documents) by taking the context of words in the text into account.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Doc2Vec Architecture  \n",
    "- Offers some form of context and can encode texts of arbitrary length into a fixed, low-dimensional, dense vector.  \n",
    "- It has found application in a wide range of NLP applications, such as text classification, document tagging, text recommendation systems, and simple chat‐\n",
    "bots for FAQs.  \n",
    "\n",
    "<img src=\"images/doc2vec.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Visualizing word Embeddings with T-SNE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### t-SNE or t-distributed Stochastic Neighboring Embedding.  \n",
    "It’s a technique used for visualizing high-dimensional data like embeddings by reducing them to two-or three-dimensional data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "t-SNE on MNIST\n",
    "\n",
    "<img src=\"images/tsne_MNIST.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "t-SNE visualization shows some interesting relationships in word_embeddings  \n",
    "<img src=\"images/tsne_relations.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Embeddings of Wikipedia articles: on various topics, obtain corresponding document vec‐\n",
    "tors for each article, then plot these vectors using t-SNE\n",
    "<img src=\"images/tsne_wikipedia.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}