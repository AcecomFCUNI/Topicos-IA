{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transfer Learning\n",
    " <img src=\"images/TransferLearning.png\" width=\"600\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13.1 BERT - 2018"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 13.1 Pretraining BERT : Masked Language Modeling (MLM) & Next Sentence Prediction (NSP)\n",
    "BERT is just a Transformer encoder that transforms the input into a series of embeddings that take\n",
    "context into account.  \n",
    "Corpus Size of text = 16 Gb  \n",
    "\n",
    " <img src=\"images/pretraining_Bert.png\" width=\"400\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 13.2 Fine Tuning BERT \n",
    "\n",
    " <img src=\"images/finetuning_Bert.png\" width=\"600\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13.2 XLNet - 2019\n",
    "\n",
    "Different pretrained process -> BERT make predictions in parallel.  \n",
    "XLNet make predictions in random order and the other predictions depends on that output.\n",
    "\n",
    "Example:\n",
    "\n",
    "“The Statue of [MASK] in New [MASK]” -> “The Statue of Liberty in New York”\n",
    "\n",
    "The Statue of [MASK] in [MASK] [MASK]\n",
    "-> “The Statue of Lincoln in Washington DC”\n",
    "                                    -> “The Statue of Liberty in New York”\n",
    "\n",
    " <img src=\"./images/xlnet_pretraining.png\" width=\"600\"/>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13.3 RoBERTa - 2019\n",
    "\n",
    "Corpus Size of text = 160 Gb  \n",
    "Remove NSP task, change batch size & other hyperparameters.  \n",
    "Obtain the state of the art on downstream tasks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13.4 ALBERT - 2019\n",
    "\n",
    "A Lite BERT.  \n",
    "One design change ALBERT makes to its model is how it handles word embeddings.\n",
    "\n",
    " <img src=\"./images/albert_embeddings.png\" width=\"600\"/>\n",
    "\n",
    "Thanks to this decomposition, ALBERT only needs to store two smaller look-up tables (V × 128, plus 128 × 768) instead of\n",
    "one big look-up table (V × 768).  \n",
    "Exm:   \n",
    "V =1000 -> 226K vs 768k  \n",
    "V = 10000 -> 1.37M vs 7.6 M\n",
    "\n",
    "Also ALBERT consider the reduction pf parameters sharing information between layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13.4 DistilBERT\n",
    "\n",
    "<img src=\"./images/bert_destilado.png\" width=\"600\"/>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13.5 ELECTRA - 2020\n",
    "\n",
    "\n",
    "<img src=\"./images/Electra.png\" width=\"600\"/>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}