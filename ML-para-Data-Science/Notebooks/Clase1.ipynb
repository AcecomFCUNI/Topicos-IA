{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scikit-learn-text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.9 64-bit ('TF1': conda)"
    }
  },
  "cells": [
    {
      "source": [
        "#  Introducci√≥n a Sklearn üß†"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSPCQGv3vEC-"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY-64THKzT21"
      },
      "source": [
        "Spam or ham (spam o no-spam)? \n",
        "\n",
        "Puede descargar el dataset [aqu√≠](https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv)\n",
        "\n",
        "#### Spam \n",
        "> Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
        "\n",
        "#### Ham\n",
        "> Oops, I'll let you know when my roommate's done\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2UGXW1pzCXG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "1791d385-adc3-4faf-d029-c848ed8b4c67"
      },
      "source": [
        "# Download the dataset\n",
        "![ ! -f spam.csv ] && wget https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-14 19:13:40--  https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503663 (492K) [text/plain]\n",
            "Saving to: ‚Äòspam.csv‚Äô\n",
            "\n",
            "\rspam.csv              0%[                    ]       0  --.-KB/s               \rspam.csv            100%[===================>] 491.86K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-10-14 19:13:40 (24.9 MB/s) - ‚Äòspam.csv‚Äô saved [503663/503663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ZJesL5zQII",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "d2547b7f-b708-4586-e291-f20b6ab641fc"
      },
      "source": [
        "spam_or_ham = pd.read_csv(\"spam.csv\", encoding='latin-1')[[\"v1\", \"v2\"]]\n",
        "spam_or_ham.columns = [\"label\", \"text\"]\n",
        "spam_or_ham.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmi_V7xDSa2Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d6b7a257-261e-4200-aea0-8af9be77d107"
      },
      "source": [
        "spam_or_ham[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H0wh8mJ_pme"
      },
      "source": [
        "## Vectorizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDctXx4_uvW"
      },
      "source": [
        "**Tokenizaci√≥n**: convertir un p√°rrafo u oraci√≥n a unidades (tokens), usualmente cada palabra es un token. \n",
        "\n",
        "En este caso, nuestra funci√≥n `tokenize` es bastante simple (e ineficiente), pero sirve para nuestros simple prop√≥sito.\n",
        "\n",
        "**Stopword removal**: eliminar tokens irrelevantes, palabras comunes y a veces signos de puntuaci√≥n.\n",
        "\n",
        "En nuestro caso, √∫nicamente estamos eliminando los s√≠mbolos de puntuaci√≥n con ayuda del set `punctuation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7aZ7JDN0gyc"
      },
      "source": [
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "def tokenize(sentence):\n",
        "    tokens = []\n",
        "    for token in sentence.split():\n",
        "        new_token = []\n",
        "        for character in token:\n",
        "            if character not in punctuation:\n",
        "                new_token.append(character.lower())\n",
        "        if new_token:\n",
        "            tokens.append(\"\".join(new_token))\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfN4KFyw89ke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5414e37a-490b-4911-e974-e1d238fc008c"
      },
      "source": [
        "tokenize(\"Go until jurong point, crazy.. \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go', 'until', 'jurong', 'point', 'crazy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veDcXo2y-hfm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a2d23727-b5a8-4245-e5e8-234f2bbdcd53"
      },
      "source": [
        "spam_or_ham.head()[\"text\"].apply(tokenize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [go, until, jurong, point, crazy, available, o...\n",
              "1                       [ok, lar, joking, wif, u, oni]\n",
              "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
              "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
              "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKo6528kD-Fe"
      },
      "source": [
        "**Stemming/Lemmatization**: Convertir cada token a su forma base: {‚Äúbiblioteca‚Äù, ‚Äúbibliotecario‚Äù, ‚Äùbibliotecas‚Äù} ‚Üí ‚Äúbibliotec‚Äù.\n",
        "\n",
        "En nuestro caso no estamos haciendo este paso, pero si es necesario, puedes revisar cosas como [NLTK - stemming](https://pythonspot.com/nltk-stemming/) o [Lemmatization Approaches with Examples in Python](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aL-Bd2PRZNy"
      },
      "source": [
        "**One-Hot encoding**: despu√©s de la tokenizaci√≥n, poner en una tabla todos los tokens en el vocabulario y por cada ocurrencia de un token en un texto, marcar con un 1 en la fila correspondiente, por ejemplo considerando las dos frases siguientes:\n",
        "\n",
        " 1. Call FREEPHONE 0800 542 0578 now!\n",
        " 2. Did you call me just now ah?\n",
        " \n",
        "Obtendr√≠amos algo como esto:\n",
        " \n",
        "|       | 0578 | 0800 | 542 | ah | call | did | freephone | just | me | now | you |\n",
        "|-------|------|------|-----|----|------|-----|-----------|------|----|-----|-----|\n",
        "| **1** | 1    | 1    | 1   | 0  | 1    | 0   | 1         | 0    | 0  | 1   | 0   |\n",
        "| **2** | 0    | 0    | 0   | 1  | 1    | 1   | 0         | 1    | 1  | 1   | 1   |  \n",
        "\n",
        "Aqu√≠ es donde entra **Scikit-Learn** a trav√©s de la clase `CountVectorizer` del m√≥dulo `sklearn.feature_extraction.text`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pdy1qPioUXE"
      },
      "source": [
        "[Slides]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uW6Pnb0_USg"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b8sjRK5Uy-X"
      },
      "source": [
        "demo_vectorizer = CountVectorizer(\n",
        "    tokenizer = tokenize,\n",
        "    binary=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdCTha5gd_J"
      },
      "source": [
        "Explicaci√≥n de los par√°metros:  \n",
        "\n",
        " - **tokenizer = tokenize**: `CountVectorizer` tiene un tokenizador por default, al pasarle nuestra funci√≥n lo estamos reemplazando con el que nosotros escribimos.  \n",
        " - **binary = True**: `CountVectorizer` por default en lugar de `1` cuenta el n√∫mero de ocurrencias de cada token, al establecer `binary = True`, le estamos indicando que no importa cuantas veces ocurra una palabra, solamente la debe contar una vez"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk4yppAjXoOM"
      },
      "source": [
        "examples = [\n",
        "    \"Call FREEPHONE 0800 542 0578 now!\",\n",
        "    \"Did you call me just now ah?\"\n",
        "]\n",
        "demo_vectorizer.fit(examples)\n",
        "vectors = demo_vectorizer.transform(examples).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkQRaxBCiLIS"
      },
      "source": [
        "Usamos `fit` y `transform` de manera separada, aunque en este caso pudimos haber usado `fit_transform`.\n",
        "\n",
        "**Nota**: usamos `toarray` para obtener un un *numpy array* ya que por default `transform` devuelve una [matriz dispersa](https://en.wikipedia.org/wiki/Sparse_matrix) que, mientras que es buena para no consumir memoria, no es tan amigable para mostrar c√≥mo es que se ven los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29hOfmHeYJf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "59bb3a4e-ad00-4ba7-964c-a7b805952d19"
      },
      "source": [
        "headers = sorted(demo_vectorizer.vocabulary_.keys())\n",
        "pd.DataFrame(vectors, columns=headers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0578</th>\n",
              "      <th>0800</th>\n",
              "      <th>542</th>\n",
              "      <th>ah</th>\n",
              "      <th>call</th>\n",
              "      <th>did</th>\n",
              "      <th>freephone</th>\n",
              "      <th>just</th>\n",
              "      <th>me</th>\n",
              "      <th>now</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0578  0800  542  ah  call  did  freephone  just  me  now  you\n",
              "0     1     1    1   0     1    0          1     0   0    1    0\n",
              "1     0     0    0   1     1    1          0     1   1    1    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DNZDSykoau-"
      },
      "source": [
        "[Slides]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk5jKAdtwF5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48d102d1-0b83-418e-9bb2-1d8a686004ac"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_text,test_text, train_labels, test_labels = train_test_split(spam_or_ham[\"text\"], \n",
        "                                                                    spam_or_ham[\"label\"],\n",
        "                                                                    stratify=spam_or_ham[\"label\"])\n",
        "print(f\"Training examples: {len(train_text)}, testing examples {len(test_text)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training examples: 4179, testing examples 1393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrLb-kHQX48C"
      },
      "source": [
        "Una vez separados los datos, ahora si podemos comenzar a entrenar nuestro algoritmo, comenzando por generar un nuevo vectorizador:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTE1NIZ5cmZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ffa6d7b-e769-43b6-a50a-0f7a66c18e30"
      },
      "source": [
        "real_vectorizer = CountVectorizer(tokenizer = tokenize, binary=True)\n",
        "\n",
        "train_X = real_vectorizer.fit_transform(train_text)\n",
        "test_X = real_vectorizer.transform(test_text)\n",
        "\n",
        "train_X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4179, 8244)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1TqRuadavKI"
      },
      "source": [
        "[Slides]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0FJRD10YcBC"
      },
      "source": [
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCFMpCooYZYY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b2516118-b1d4-40b7-8d68-d6febe86a4ec"
      },
      "source": [
        "classifier = LinearSVC()\n",
        "classifier.fit(train_X, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z39iEx8Ta49W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "666251e8-fbcb-461e-bb46-90385f926338"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predicciones = classifier.predict(test_X)\n",
        "\n",
        "accuracy = accuracy_score(test_labels, predicciones)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4%}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 98.4925%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et4yRO75emkm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26oC5B26enC7"
      },
      "source": [
        "### Predicciones en nuevos datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv3yK8XTbl5x"
      },
      "source": [
        "spam = \"Want to win FREE tickets to a football match? txt WIN\"\n",
        "ham = \"Do you want to go to a football match with me?\"\n",
        "\n",
        "examples = [\n",
        "    spam,\n",
        "    ham\n",
        "]\n",
        "\n",
        "examples_X = real_vectorizer.transform(examples)\n",
        "predicciones = classifier.predict(examples_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbTCIYTVjJ8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6eccec1c-b896-4565-c025-bf38899cac94"
      },
      "source": [
        "for text, label in zip(examples, predicciones):\n",
        "    print(f\"{label:5} - {text}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spam  - Want to win FREE tickets to a football match? txt WIN\n",
            "ham   - Do you want to go to a football match with me?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsg-6TmJjuyt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}